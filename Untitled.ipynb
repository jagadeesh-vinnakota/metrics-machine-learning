{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "#importing the data set\n",
    "dataset = pd.read_csv('D:\\datascience-practice\\insurance.csv')\n",
    "\n",
    "x = dataset.iloc[:,:-1].values\n",
    "y = dataset.iloc[:,6].values\n",
    "\n",
    "#Encoding the categorical data\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "\n",
    "#for encoding places\n",
    "labelencoder = LabelEncoder()\n",
    "x[:, 5] = labelencoder.fit_transform(x[:, 5])\n",
    "\n",
    "#for encoding smoking\n",
    "labelencoder_place = LabelEncoder()\n",
    "x[:, 4] = labelencoder_place.fit_transform(x[:, 4])\n",
    "\n",
    "#for encoding gender\n",
    "labelencoder_gender = LabelEncoder()\n",
    "x[:, 1] = labelencoder_place.fit_transform(x[:, 1])\n",
    "\n",
    "#converting the region attributes\n",
    "onehotencoder = OneHotEncoder(categorical_features = [5])\n",
    "x = onehotencoder.fit_transform(x).toarray()\n",
    "#Avoding the dummy variable trap\n",
    "x = x[:, 1:]\n",
    "\n",
    "\n",
    "#converting the smoking attributes\n",
    "onehotencoder_smoke = OneHotEncoder(categorical_features = [7])\n",
    "x = onehotencoder_smoke.fit_transform(x).toarray()\n",
    "x = x[:, 1:]\n",
    "\n",
    "#converting the gender attributes\n",
    "onehotencoder_gender = OneHotEncoder(categorical_features = [5])\n",
    "x = onehotencoder_gender.fit_transform(x).toarray()\n",
    "x = x[:, 1:]\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc_x = StandardScaler()\n",
    "sc_y = StandardScaler()\n",
    "\n",
    "x = sc_x.fit_transform(x)\n",
    "\n",
    "y = sc_y.fit_transform(y.reshape(-1,1))\n",
    "\n",
    "x = np.delete(x,1,axis=1)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.2, random_state = 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "  Our task is regression where \n",
    "  we are predicting the insurance amount claimed by people depending \n",
    "  on their age, their location and other features. \n",
    "  \n",
    "  We trained a model using Linear regression.\n",
    "  \n",
    "'''\n",
    "from sklearn.linear_model import LinearRegression\n",
    "regresser = LinearRegression()\n",
    "regresser.fit(x_train, y_train)\n",
    "\n",
    "#predicting the test set results\n",
    "y_pred = regresser.predict(x_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.16434435894991628"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "\n",
    "  After building a machine learning model, we need to validate the model using some metrics.\n",
    "   For regression tasks, these are the metrics we generally used.\n",
    "\n",
    "  1. R squared error: It measures how well our data fit into the model and how \n",
    "  accurately the model is predicting the unseen data.\n",
    "\n",
    "     R^2 = 1 - (SSE/TSS) \n",
    "               where SSE is defined as the sum of squared errors (actual target - predicted target)\n",
    "               where TSS is defined as the total sum of squares (Actual target - mean of target)\n",
    " The possible values of this be 0 to 1.The more closer to zero, the better the model is.\n",
    "\n",
    "'''\n",
    "# Here is the python code snippet to calculate R^2 error \n",
    "regresser.score(x_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.248148025811295"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "'''\n",
    " 2. Max error metric: defines as the max difference between the true target and predicted target.\n",
    "    Lower the value of max error, better the model is\n",
    "\n",
    "'''\n",
    "# Here is the python code snippet to calculate max error metric\n",
    "arr = np.setdiff1d(y_test,y_pred)\n",
    "arr = np.absolute(arr)\n",
    "np.max(arr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9525931660665768"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "  3. Root Mean Squared Error(RMSE): Defined as the square root of mean squared error. \n",
    "     where mean suared error is defined the mean of sum of the square difference \n",
    "     between actual target and predicted target.\n",
    "\n",
    " RMSE is a measure of the average deviation of the estimates from the observed values. \n",
    "  Lower the value of RMSE, better the model is.\n",
    "\n",
    "'''\n",
    "# Here is the python code snippet to calculate RMSE\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import math\n",
    "math.sqrt(mean_squared_error(y_test,y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "#importing the data set\n",
    "Dataset = pd.read_csv('D:\\datascience-practice\\diabetes.csv')\n",
    "\n",
    "Dataset.corr()\n",
    "X = Dataset.iloc[:,:-1].values\n",
    "#x = x[:,1:]\n",
    "Y = Dataset.iloc[:,8].values\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.2, random_state = 0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    " Now lets see the metrics required for validating a classification model. \n",
    " We are building a classifier which predicts wether a person have diabetes \n",
    " or depending on the features like age, gender, blood pressure and many more.\n",
    " \n",
    " Classifier is constructed on data using Logistic Regression.\n",
    " \n",
    "'''\n",
    "\n",
    "#Logistic Regression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "lr = LogisticRegression()\n",
    "lr.fit(X_train,Y_train)\n",
    "Y_pred = lr.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8181818181818182"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "    After building a machine learning model, we need to validate the model using some metrics.\n",
    "    For classification tasks, these are the metrics we generally used.\n",
    "\n",
    "    1. Accuracy: It a calculated as the number of samples correctly classified by \n",
    "       classifier to the total number of samples.\n",
    "       \n",
    "            Accuracy = (Number of correctly identified samples)/(Total samples)\n",
    "            \n",
    "       Possible values of accuracy are between 0 to 1. Higher the value, the better\n",
    "          the classifier performs.\n",
    "\n",
    "'''\n",
    "\n",
    "# Here is the python code snippet for calculating accuracy of a classifier\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(Y_test,Y_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[98,  9],\n",
       "       [19, 28]], dtype=int64)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "  \n",
    "  2. Confusion Matrix: From this we can know the numbers of how many samples are \n",
    "    correctly classified and how many are mis classified.\n",
    "    \n",
    "    The structure of confusion matrix is like this \n",
    "    \n",
    "    [ \n",
    "      [true positive, false positive],\n",
    "      [false negative, true negative] \n",
    "    ]\n",
    "    \n",
    "    True positive: Actual value is positive, predicted as positive\n",
    "    False Positive: Actual value is negative, but predicted as positive\n",
    "    False Negative: Actual value is positive, but predicted as negative\n",
    "    True Negative: Actual value is negative, predicted as negative.\n",
    "    \n",
    "    correctly classified samples:     true positive + true negative\n",
    "    In correctly classified samples:  false positive + false negative\n",
    "    \n",
    "'''\n",
    "\n",
    "# Here is the python code snippet for calculating confusion matrix\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix(Y_test,Y_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7567567567567568"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "    3. Precision Score: Defined as the ratio of true positive to the sum of \n",
    "          true positive and false positive.\n",
    "       The precision is intuitively the ability of the classifier not to label as positive \n",
    "         a sample that is negative.\n",
    "         \n",
    "         Possible values of precision are 0 to 1. Greater the value,\n",
    "            better the model is.\n",
    "\n",
    "'''\n",
    "# Python code snippet for calculating precision\n",
    "\n",
    "from sklearn.metrics import precision_score\n",
    "precision_score(Y_test,Y_pred)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5957446808510638"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "    4. Recall Score: Defined as the ratio of true positive to the sum of \n",
    "          true positive and false negative.\n",
    "       \n",
    "       The recall is intuitively the ability of the classifier to \n",
    "         find all the positive samples.\n",
    "         \n",
    "      Possible values of recall are 0 to 1. Greater the value,\n",
    "            better the model is.\n",
    "\n",
    "'''\n",
    "\n",
    "# python code snippet to calculate the recall score\n",
    "from sklearn.metrics import recall_score\n",
    "recall_score(Y_test,Y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6666666666666666"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "\n",
    "   5. F1-score: Can be interpreted as a weighted average of the precision and recall.\n",
    "   \n",
    "        f1-score = 2 * (precision * recall) / (precision + recall)\n",
    "        \n",
    "        Possible values of f1-score are 0 to 1. Greater the value,\n",
    "            better the model is.      \n",
    "\n",
    "'''\n",
    "#python code snippet to calculate the f1-score\n",
    "from sklearn.metrics import f1_score\n",
    "f1_score(Y_test,Y_pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
